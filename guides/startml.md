
<!-- 
---
layout: default
published : false
title: Getting Started With ML
permalink: /guides/startml/  
hidden : true
---
Getting Started With ML -->
<h1> Beginners Guide to Machine Learning </h1>

Well it got redundent over time explaning the same thing to juniors, and I kept on missing out on stuff, so here's a comprehensive guide on how one should approach Machine Learning.

<h3> Table of Content </h3>
1. [What I Usually Suggest](#WhatIUsuallySuggest)
2. Jump over to a [structured topicwise guide inspired by IITK](#iitkGuide)
3. Here's a [simple guide from the 'Introduction to ML'](https://hackmd.io/@aryan1113/BkdlTOpkn) session from Jan/Feb 2023, and the [session notes](https://hackmd.io/@aryan1113/Byh94FUCs) as well.

<div id="WhatIUsuallySuggest"></div>
<h2>
What I Usually Suggest
</h2>

Assuming you are an eager freshman / sophomore, you should start simply by Visualizing common datasets, which is formally referred to as Exploratory Data Analysis.

- If you have heard of [The Prog Club](https://github.com/BitByte-TPC/), chances are they'll have a summer course for this as well, which started as a bootcamp in the summers of 2023, aimed at freshmen transitioning into sophomores.

- Read about eigenvalues, eigenvectors (class 12 topics, for intuition check out a visualization by 3Blue1Brown)

- Read about PRP Probability and Random Processes, I took a course by Prof. SNS in my sophomore year which helped me cement ideas of Random Variables and Expectations of RV.

- Revise basic Probability and Statistics concepts as well, to know about the different distributions out there and some universal laws like the Central Limit Theorem.

Keep experimenting with datasets, find outliers, missing data, basic information about the data from visualizing trends across variables.

<div id="iitkGuide"></div>
<h2>
Here's a guide inspired by a introductory course to ML from IITK
</h2>

<h3>
Preliminaries
</h3>
Multivariate calculus:  gradient, Hessian, Jacobian, chain rule <br>
Linear algebra:  determinants, eigenvalues/vectors, SVD <br>
Probability theory:  conditional probability, marginal probability, Bayes rule <br>

<h3>
Supervised Learning 
</h3>

nearest-neighbors, decision trees <br>
Learning by function approximation <br>
Linear models:  (multiclass) support vector machines, ridge regression <br>
Non-linear models:  kernel methods, neural networks (feedforward) <br>
Learning by probabilistic modeling <br>
Discriminative methods:  (multiclass) logistic regression, generalized linear models <br>
Generative methods:  naive Bayes <br>

<h3>
Unsupervised Learning
</h3>

Discriminative Models: k-means (clustering), PCA (dimensionality reduction) <br>
Generative Models 
Latent variable models:  expectation-maximization for learning latent variable models <br>
Applications:  Gaussian mixture models, probabilistic PCA <br>
Concepts of over-fitting and generalization, bias-variance tradeoffs <br>
Optimization for machine learning:  (stochastic/mini-batch) gradient descent <br>

<h3>
How to proceed ?
</h3>

Ensemble methods:  boosting, bagging, random forests <br>
Deep learning:  CNN, RNN, LSTM, autoencoders
